T. Prise en main

Testez la commande "src/examples/example_BFS DT -h2 -v"

Sortie :

❯ src/examples/example_BFS DT -h2 -v
ArgumentUtils: Problem DecTiger instantiated.
BruteForceSearchPlanner initialized

"""
value=8
JointPolicyPureVector: 
JPolComponent_VectorImplementation index 0
Policy for agent 0 (index 0):
Oempty,  --> a00:Listen
Oempty, o00:HearLeft,  --> a00:Listen
Oempty, o01:HearRight,  --> a00:Listen
Policy for agent 1 (index 0):
Oempty,  --> a10:Listen
Oempty, o10:HearLeft,  --> a10:Listen
Oempty, o11:HearRight,  --> a10:Listen
"""

Quelques précisions sur la commande :
- BFS = Brute Force Search
- DT = DecTiger problem
- h2 = hauteur 2 dans l'arbre
- v = verbose

q. Analysez la politique obtenue

>> recomposer l'arbre de décision (la politique)

agent 0:

                     a0:Listen        
                          │           
                 o0:HL ───┴── o1:HR   
                   │             │    
               a0:Listen     a0:Listen

agent 1:

                     a0:Listen        
                          │           
                 o0:HL ───┴── o1:HR   
                   │             │    
               a0:Listen     a0:Listen

La politique obtenue est assez simple : sans informations, on se contente d'écouter deux fois.

q. Résolvez le problème à l'horizon 3. Que constate-t-on en ce qui concerne le temps de résolution ? Comparez la politique obtenue avec celle à l'horizon 2.

Problème à l'horizon 3 :

"""
❯ src/examples/example_BFS DT -h3 -v
Policy for agent 0 (index 0):
Oempty,  --> a00:Listen
Oempty, o00:HearLeft,  --> a00:Listen
Oempty, o01:HearRight,  --> a00:Listen
Oempty, o00:HearLeft, o00:HearLeft,  --> a00:Listen
Oempty, o00:HearLeft, o01:HearRight,  --> a00:Listen
Oempty, o01:HearRight, o00:HearLeft,  --> a00:Listen
Oempty, o01:HearRight, o01:HearRight,  --> a00:Listen
Policy for agent 1 (index 0):
Oempty,  --> a10:Listen
Oempty, o10:HearLeft,  --> a10:Listen
Oempty, o11:HearRight,  --> a10:Listen
Oempty, o10:HearLeft, o10:HearLeft,  --> a10:Listen
Oempty, o10:HearLeft, o11:HearRight,  --> a10:Listen
Oempty, o11:HearRight, o10:HearLeft,  --> a10:Listen
Oempty, o11:HearRight, o11:HearRight,  --> a10:Listen
"""

Temps d'exécution :
- h2 : 0.016s
- h3 : 19.731s

Le temps d'exécution explose totalement.

Comparaison des politiques :

agent 0:                                             
                                                           
                           a0:Listen                       
                                │                          
                 o0:HL ─────────┴───────── o1:HR           
                   │                          │            
               a0:Listen                  a0:Listen        
          o0:HL ───┴── o1:HR         o0:HL ───┴── o1:HR             
            │             │            │             │     
        a0:Listen     a0:Listen    a0:Listen     a0:Listen                                 
agent 1:                                             
                                                           
                           a0:Listen                       
                                │                          
                 o0:HL ─────────┴───────── o1:HR           
                   │                          │            
               a0:Listen                  a0:Listen        
          o0:HL ───┴── o1:HR         o0:HL ───┴── o1:HR    
            │             │            │             │     
        a0:Listen     a0:Listen    a0:Listen     a0:Listen 

La politique est très similaire : on continue à écouter indépendamment des informations reçues. Ca s'explique par le risque d'ouvrir la porte : les agents continuent d'observer jusqu'à avoir assez d'informations pour réduire le risque. Celà s'observera probablement sur une hauteur supérieure, avec des situations où l'agent est assez certain poru agir opposées à des situations où l'incertitude est toujours présente.

q. Résolvez le problème de manière approchée en utilisant la commande -----

"""
Policy for agent 0 (index 55):
() --> a00:Listen
(o00:HearLeft) --> a00:Listen
(o01:HearRight) --> a00:Listen
(o00:HearLeft,o00:HearLeft) --> a02:OpenRight
(o00:HearLeft,o01:HearRight) --> a00:Listen
(o01:HearRight,o00:HearLeft) --> a00:Listen
(o01:HearRight,o01:HearRight) --> a01:OpenLeft
Policy for agent 1 (index 55):
() --> a10:Listen
(o10:HearLeft) --> a10:Listen
(o11:HearRight) --> a10:Listen
(o10:HearLeft,o10:HearLeft) --> a12:OpenRight
(o10:HearLeft,o11:HearRight) --> a10:Listen
(o11:HearRight,o10:HearLeft) --> a10:Listen
(o11:HearRight,o11:HearRight) --> a11:OpenLeft
"""


agent 0:                                             
                                                           
                           a0:Listen                       
                                │                          
                 o0:HL ─────────┴───────── o1:HR           
                   │                          │            
               a0:Listen                  a0:Listen        
          o0:HL ───┴── o1:HR         o0:HL ───┴── o1:HR             
            │             │            │             │     
      a0:OpenRight    a0:Listen    a0:Listen    a0:OpenLeft

agent 1:                                             
                                                           
                           a0:Listen                       
                                │                          
                 o0:HL ─────────┴───────── o1:HR           
                   │                          │            
               a0:Listen                  a0:Listen        
          o0:HL ───┴── o1:HR         o0:HL ───┴── o1:HR             
            │             │            │             │     
      a0:OpenRight    a0:Listen    a0:Listen    a0:OpenLeft

Dans cette politique, on prend une décision dans la situation où l'on entend deux fois de suite le tigre au même endroit. Cette politique n'est pas la politique optimale, on peut donc supposer que l'algorithme approché n'a pas rencontré la politique optimale dans son exploration des possibilités.

On peut prédire assez facilement le moment où agir devient plus intéressant que "Listen":

listen-listen : -2
ouvrir la bonne porte : 9 ou 20 ou -100 (en fonction de la coordination)

On attend donc que les observations nous donnent une proba suffisante pour agir.

q. testez la résolution à horizon 5 avec la méthode exacte exhaustive et avec GMAA. Que constatez-vous ?

Méthode exhaustive :
Jpol # 5000 of 5069619362125685561 - 9.863e-14%
Jpol # 6000 of 5069619362125685561 - 1.184e-13%

Dès les premiers secondes, on se rend compte que la progression est extrêmement lente et l'on ne pourra pas explorer l'arbre des possibles dans un temps raisonnable.

Méthode approchée :

"""
❯ ./GMAA -G FSPC -B AM -v -h5 DT -v
Policy for agent 0 (index 15533624506455):
() --> a00:Listen
(o00:HearLeft) --> a00:Listen
(o01:HearRight) --> a00:Listen
(o00:HearLeft,o00:HearLeft) --> a02:OpenRight
(o00:HearLeft,o01:HearRight) --> a00:Listen
(o01:HearRight,o00:HearLeft) --> a00:Listen
(o01:HearRight,o01:HearRight) --> a01:OpenLeft
(o00:HearLeft,o00:HearLeft,o00:HearLeft) --> a00:Listen
(o00:HearLeft,o00:HearLeft,o01:HearRight) --> a00:Listen
(o00:HearLeft,o01:HearRight,o00:HearLeft) --> a00:Listen
(o00:HearLeft,o01:HearRight,o01:HearRight) --> a00:Listen
(o01:HearRight,o00:HearLeft,o00:HearLeft) --> a00:Listen
(o01:HearRight,o00:HearLeft,o01:HearRight) --> a00:Listen
(o01:HearRight,o01:HearRight,o00:HearLeft) --> a00:Listen
(o01:HearRight,o01:HearRight,o01:HearRight) --> a00:Listen
(o00:HearLeft,o00:HearLeft,o00:HearLeft,o00:HearLeft) --> a00:Listen
(o00:HearLeft,o00:HearLeft,o00:HearLeft,o01:HearRight) --> a00:Listen
(o00:HearLeft,o00:HearLeft,o01:HearRight,o00:HearLeft) --> a00:Listen
(o00:HearLeft,o00:HearLeft,o01:HearRight,o01:HearRight) --> a00:Listen
(o00:HearLeft,o01:HearRight,o00:HearLeft,o00:HearLeft) --> a00:Listen
(o00:HearLeft,o01:HearRight,o00:HearLeft,o01:HearRight) --> a00:Listen
(o00:HearLeft,o01:HearRight,o01:HearRight,o00:HearLeft) --> a00:Listen
(o00:HearLeft,o01:HearRight,o01:HearRight,o01:HearRight) --> a00:Listen
(o01:HearRight,o00:HearLeft,o00:HearLeft,o00:HearLeft) --> a00:Listen
(o01:HearRight,o00:HearLeft,o00:HearLeft,o01:HearRight) --> a00:Listen
(o01:HearRight,o00:HearLeft,o01:HearRight,o00:HearLeft) --> a00:Listen
(o01:HearRight,o00:HearLeft,o01:HearRight,o01:HearRight) --> a00:Listen
(o01:HearRight,o01:HearRight,o00:HearLeft,o00:HearLeft) --> a00:Listen
(o01:HearRight,o01:HearRight,o00:HearLeft,o01:HearRight) --> a00:Listen
(o01:HearRight,o01:HearRight,o01:HearRight,o00:HearLeft) --> a00:Listen
(o01:HearRight,o01:HearRight,o01:HearRight,o01:HearRight) --> a00:Listen
Policy for agent 1 (index 15533624506455):
() --> a10:Listen
(o10:HearLeft) --> a10:Listen
(o11:HearRight) --> a10:Listen
(o10:HearLeft,o10:HearLeft) --> a12:OpenRight
(o10:HearLeft,o11:HearRight) --> a10:Listen
(o11:HearRight,o10:HearLeft) --> a10:Listen
(o11:HearRight,o11:HearRight) --> a11:OpenLeft
(o10:HearLeft,o10:HearLeft,o10:HearLeft) --> a10:Listen
(o10:HearLeft,o10:HearLeft,o11:HearRight) --> a10:Listen
(o10:HearLeft,o11:HearRight,o10:HearLeft) --> a10:Listen
(o10:HearLeft,o11:HearRight,o11:HearRight) --> a10:Listen
(o11:HearRight,o10:HearLeft,o10:HearLeft) --> a10:Listen
(o11:HearRight,o10:HearLeft,o11:HearRight) --> a10:Listen
(o11:HearRight,o11:HearRight,o10:HearLeft) --> a10:Listen
(o11:HearRight,o11:HearRight,o11:HearRight) --> a10:Listen
(o10:HearLeft,o10:HearLeft,o10:HearLeft,o10:HearLeft) --> a10:Listen
(o10:HearLeft,o10:HearLeft,o10:HearLeft,o11:HearRight) --> a10:Listen
(o10:HearLeft,o10:HearLeft,o11:HearRight,o10:HearLeft) --> a10:Listen
(o10:HearLeft,o10:HearLeft,o11:HearRight,o11:HearRight) --> a10:Listen
(o10:HearLeft,o11:HearRight,o10:HearLeft,o10:HearLeft) --> a10:Listen
(o10:HearLeft,o11:HearRight,o10:HearLeft,o11:HearRight) --> a10:Listen
(o10:HearLeft,o11:HearRight,o11:HearRight,o10:HearLeft) --> a10:Listen
(o10:HearLeft,o11:HearRight,o11:HearRight,o11:HearRight) --> a10:Listen
(o11:HearRight,o10:HearLeft,o10:HearLeft,o10:HearLeft) --> a10:Listen
(o11:HearRight,o10:HearLeft,o10:HearLeft,o11:HearRight) --> a10:Listen
(o11:HearRight,o10:HearLeft,o11:HearRight,o10:HearLeft) --> a10:Listen
(o11:HearRight,o10:HearLeft,o11:HearRight,o11:HearRight) --> a10:Listen
(o11:HearRight,o11:HearRight,o10:HearLeft,o10:HearLeft) --> a10:Listen
(o11:HearRight,o11:HearRight,o10:HearLeft,o11:HearRight) --> a10:Listen
(o11:HearRight,o11:HearRight,o11:HearRight,o10:HearLeft) --> a10:Listen
(o11:HearRight,o11:HearRight,o11:HearRight,o11:HearRight) --> a10:Listen
"""

C'est la même politique qu'à l'horizon 3, sauf qu'on listen dans tous les cas suivants.

On l'obtient en 1.909 secondes, c'est à dire 10 fois plus vite qu'un horizon 3 exact.

T. Patrouille multiagent - totalement observable

q. testez la résolution du problème de rencontre sur une grille (Small Grid) à l'horizon 3. Quelle stratégie est obtenue ?

test à l'horizon 3 :

"""
action policy : [16,25]((2.19116,1.64818,2.19116,1.64818,2.40835,1.64818,1.85931,1.64818,1.40675,1.65424,2.19116,1.64818,2.19116,1.64818,2.40835,1.64818,1.40675,1.64818,1.85931,1.65424,2.40835,1.65424,2.40835,1.65424,2.71),(1.39476,1.21365,1.93775,1.39476,1.32239,1.21365,1.18354,1.45508,1.21365,1.17139,1.39476,1.21365,1.93775,1.39476,1.32239,1.93775,1.45508,1.72661,1.93775,2.07651,1.32239,1.17139,2.07651,1.32239,1.20177),(1.93775,1.39476,1.39476,1.21365,1.32239,1.72661,1.93775,1.93775,1.45508,2.07651,1.93775,1.39476,1.39476,1.21365,1.32239,1.45508,1.21365,1.21365,1.18354,1.17139,2.07651,1.32239,1.32239,1.17139,1.20177),(1.21372,1.03261,1.21372,1.03261,0.96016,1.24383,1.21372,1.69639,1.21372,1.11117,1.21372,1.03261,1.21372,1.03261,0.96016,1.69639,1.21372,1.24383,1.21372,1.11117,1.11117,0.96016,1.11117,0.96016,0.899757),(1.39476,1.21365,1.39476,1.93775,1.32239,1.21365,1.18354,1.21365,1.45508,1.17139,1.93775,1.45508,1.93775,1.72661,2.07651,1.39476,1.21365,1.39476,1.93775,1.32239,1.32239,1.17139,1.32239,2.07651,1.20177),(2.19116,1.64818,1.64818,2.19116,2.40835,1.64818,1.85931,1.40675,1.64818,1.65424,1.64818,1.40675,1.85931,1.64818,1.65424,2.19116,1.64818,1.64818,2.19116,2.40835,2.40835,1.65424,1.65424,2.40835,2.71),(1.21372,1.03261,1.03261,1.21372,0.96016,1.24383,1.21372,1.21372,1.69639,1.11117,1.69639,1.21372,1.21372,1.24383,1.11117,1.21372,1.03261,1.03261,1.21372,0.96016,1.11117,0.96016,0.96016,1.11117,0.899757),(1.93775,1.39476,1.21365,1.39476,1.32239,1.72661,1.93775,1.45508,1.93775,2.07651,1.45508,1.21365,1.18354,1.21365,1.17139,1.93775,1.39476,1.21365,1.39476,1.32239,2.07651,1.32239,1.17139,1.32239,1.20177),(1.93775,1.72661,1.93775,1.45508,2.07651,1.39476,1.93775,1.39476,1.21365,1.32239,1.39476,1.93775,1.39476,1.21365,1.32239,1.21365,1.45508,1.21365,1.18354,1.17139,1.32239,2.07651,1.32239,1.17139,1.20177),(1.21372,1.24383,1.69639,1.21372,1.11117,1.03261,1.21372,1.21372,1.03261,0.96016,1.03261,1.21372,1.21372,1.03261,0.96016,1.21372,1.69639,1.24383,1.21372,1.11117,0.96016,1.11117,1.11117,0.96016,0.899757),(1.85931,1.64818,1.64818,1.40675,1.65424,1.64818,2.19116,2.19116,1.64818,2.40835,1.64818,2.19116,2.19116,1.64818,2.40835,1.40675,1.64818,1.64818,1.85931,1.65424,1.65424,2.40835,2.40835,1.65424,2.71),(1.18354,1.21365,1.45508,1.21365,1.17139,1.21365,1.39476,1.93775,1.39476,1.32239,1.21365,1.39476,1.93775,1.39476,1.32239,1.45508,1.93775,1.72661,1.93775,2.07651,1.17139,1.32239,2.07651,1.32239,1.20177),(1.21372,1.24383,1.21372,1.69639,1.11117,1.03261,1.21372,1.03261,1.21372,0.96016,1.21372,1.69639,1.21372,1.24383,1.11117,1.03261,1.21372,1.03261,1.21372,0.96016,0.96016,1.11117,0.96016,1.11117,0.899757),(1.93775,1.72661,1.45508,1.93775,2.07651,1.39476,1.93775,1.21365,1.39476,1.32239,1.21365,1.45508,1.18354,1.21365,1.17139,1.39476,1.93775,1.21365,1.39476,1.32239,1.32239,2.07651,1.17139,1.32239,1.20177),(1.18354,1.21365,1.21365,1.45508,1.17139,1.21365,1.39476,1.39476,1.93775,1.32239,1.45508,1.93775,1.93775,1.72661,2.07651,1.21365,1.39476,1.39476,1.93775,1.32239,1.17139,1.32239,1.32239,2.07651,1.20177),(1.85931,1.64818,1.40675,1.64818,1.65424,1.64818,2.19116,1.64818,2.19116,2.40835,1.40675,1.64818,1.85931,1.64818,1.65424,1.64818,2.19116,1.64818,2.19116,2.40835,1.65424,2.40835,1.65424,2.40835,2.71))Simulating policy with nrRuns: 1000 and seed: 42
Avg rewards: < 1.68966 >
"""

Stratégie obtenue --> analyser la matrice et trouver pour chaque état l'action qui est préconisée.

On a un Avg rewards de 1.7 environ contre 8.9 avec l'horizon infini.

----
Coller la matrice :

      (↑ ↑) (↑ ↓) (↑ ←) (↑ →) (↑ •) (↓ ↑) (↓ ↓) (↓ ←) (↓ →) (↓ •) (← ↑) (← ↓) (← ←) (← →) (← •) (→ ↑) (→ ↓) (→ ←) (→ →) (→ •) (• ↑) (• ↓) (• ←) (• →) (• •)
(◉___) 2.19  1.65  2.19  1.65  2.41  1.65  1.86  1.65  1.41  1.65  2.19  1.65  2.19  1.65  2.41  1.65  1.41  1.65  1.86  1.65  2.41  1.65  2.41  1.65  2.71
(xo__) 1.39  1.21  1.94  1.39  1.32  1.21  1.18  1.46  1.21  1.17  1.39  1.21  1.94  1.39  1.32  1.94  1.46  1.73  1.94  2.08  1.32  1.17  2.08  1.32   1.2
(x_o_) 1.94  1.39  1.39  1.21  1.32  1.73  1.94  1.94  1.46  2.08  1.94  1.39  1.39  1.21  1.32  1.46  1.21  1.21  1.18  1.17  2.08  1.32  1.32  1.17   1.2
(x__o) 1.21  1.03  1.21  1.03  0.96  1.24  1.21   1.7  1.21  1.11  1.21  1.03  1.21  1.03  0.96   1.7  1.21  1.24  1.21  1.11  1.11  0.96  1.11  0.96   0.9
(ox__) 1.39  1.21  1.39  1.94  1.32  1.21  1.18  1.21  1.46  1.17  1.94  1.46  1.94  1.73  2.08  1.39  1.21  1.39  1.94  1.32  1.32  1.17  1.32  2.08   1.2
(_◉__) 2.19  1.65  1.65  2.19  2.41  1.65  1.86  1.41  1.65  1.65  1.65  1.41  1.86  1.65  1.65  2.19  1.65  1.65  2.19  2.41  2.41  1.65  1.65  2.41  2.71
(_xo_) 1.21  1.03  1.03  1.21  0.96  1.24  1.21  1.21   1.7  1.11   1.7  1.21  1.21  1.24  1.11  1.21  1.03  1.03  1.21  0.96  1.11  0.96  0.96  1.11   0.9
(_x_o) 1.94  1.39  1.21  1.39  1.32  1.73  1.94  1.46  1.94  2.08  1.46  1.21  1.18  1.21  1.17  1.94  1.39  1.21  1.39  1.32  2.08  1.32  1.17  1.32   1.2
(o_x_) 1.94  1.73  1.94  1.46  2.08  1.39  1.94  1.39  1.21  1.32  1.39  1.94  1.39  1.21  1.32  1.21  1.46  1.21  1.18  1.17  1.32  2.08  1.32  1.17   1.2
(_ox_) 1.21  1.24   1.7  1.21  1.11  1.03  1.21  1.21  1.03  0.96  1.03  1.21  1.21  1.03  0.96  1.21   1.7  1.24  1.21  1.11  0.96  1.11  1.11  0.96   0.9
(__◉_) 1.86  1.65  1.65  1.41  1.65  1.65  2.19  2.19  1.65  2.41  1.65  2.19  2.19  1.65  2.41  1.41  1.65  1.65  1.86  1.65  1.65  2.41  2.41  1.65  2.71
(__xo) 1.18  1.21  1.46  1.21  1.17  1.21  1.39  1.94  1.39  1.32  1.21  1.39  1.94  1.39  1.32  1.46  1.94  1.73  1.94  2.08  1.17  1.32  2.08  1.32   1.2
(o__x) 1.21  1.24  1.21   1.7  1.11  1.03  1.21  1.03  1.21  0.96  1.21   1.7  1.21  1.24  1.11  1.03  1.21  1.03  1.21  0.96  0.96  1.11  0.96  1.11   0.9
(_o_x) 1.94  1.73  1.46  1.94  2.08  1.39  1.94  1.21  1.39  1.32  1.21  1.46  1.18  1.21  1.17  1.39  1.94  1.21  1.39  1.32  1.32  2.08  1.17  1.32   1.2
(__ox) 1.18  1.21  1.21  1.46  1.17  1.21  1.39  1.39  1.94  1.32  1.46  1.94  1.94  1.73  2.08  1.21  1.39  1.39  1.94  1.32  1.17  1.32  1.32  2.08   1.2
(___◉) 1.86  1.65  1.41  1.65  1.65  1.65  2.19  1.65  2.19  2.41  1.41  1.65  1.86  1.65  1.65  1.65  2.19  1.65  2.19  2.41  1.65  2.41  1.65  2.41  2.71


0 --> 24 [2.71]
 1 --> 19 [2.07651]
 2 -->  9 [2.07651]
 3 -->  7 [1.69639]
 4 --> 14 [2.07651]
 5 --> 24 [2.71]
 6 -->  8 [1.69639]
 7 -->  9 [2.07651]
 8 -->  4 [2.07651]
 9 -->  2 [1.69639]
10 --> 24 [2.71]
11 --> 19 [2.07651]
12 -->  3 [1.69639]
13 -->  4 [2.07651]
14 --> 14 [2.07651]
15 --> 24 [2.71]

Ajouter les tests de cohérence :
en state 0, on choisit l'action 24 car c'est (stay, stay)
dans les state en diagonale (3, 6, 9, 12), on a deux actions équivalentes (car l'état de la grille est symétrique).

Rappel: ici on est dans le problème où on cherche à aller sur la même case

+ analyser les choix faits pour chaque action

q. Modifiez le fihcier de description du problème de rencontre sur une grille pour formaliser le problème de patrouille dans le cas totalement observable.

penser à modifier les perceptions observables (ex: nnnnnnynnn -> LW - LeftWall)
on passe en 4 actions par agent, soit 16 actions au total

q. Analysez la politique obtenue.

--

T. Patrouille multiagent - partiellement observable

q. Adaptez la formulation du problème pour traiter le cas partiellement observable

??

q. Résolvez le problème en utilisant GMAA. Faites varier les options de résolution ainsi que l'horizon. Analysez les résultats.


